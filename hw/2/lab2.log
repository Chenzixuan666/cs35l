sorted /usr/share/dict/words using the command:
    sort -o words /usr/share/dict/words

obtained a copy of the assignment web page with:
    wget http://www.cs.ucla.edu/classes/winter15/cs35L/assign/assign2.html

tr -c 'A-Za-z' '[\n*]'
    replaces each non-alphabetic character with a newline

tr -cs 'A-Za-z' '[\n*]'
    replaces each group of contiguous non-alphabetic characters with a newline

tr -cs 'A-Za-z' '[\n*]' | sort
    alphabetically sorts the words (groups of contiguous alphabetic
    characters) in the file, with each word on its own line

tr -cs 'A-Za-z' '[\n*]' | sort -u
    does the same as the previous command, but de-dupes the output; i.e. each
    word is only displayed once

tr -cs 'A-Za-z' '[\n*]' | sort -u | comm - words
    prints three columns - words only in the HTML page, words only in the
    wordlist, and words in common between the two

tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - words
    prints words in the HTML page that are not in the wordlist

I used wget to get the file at http://mauimapp.com/moolelo/hwnwdseng.htm.
The first thing to do was to extract the table from the surrounding HTML.
I accomplished this by grepping for '<td>.*</td>' to get all table cell
elements. This worked because there is only one table on the page, and
assuming valid HTML, <td> elements can only be inside a table. Next, to
simplify removing opening and closing tags, I removed all forward slashes from
the file with sed. Next, I used sed to remove the <td>s and empty lines. I
then used awk, passing it NR % 2 == 0 to extract only even-numbered lines, as
even lines contained the Hawaiian words we were interested in retaining. Next,
I removed underlines and replaced backticks with apostrophes using sed. tr
handled lowercasing everything. Then, I used sed to remove commas and replace
all whitespace characters with newlines. I used a grep regex with an
inverse matching group composed of the valid Hawaiian characters, which
matched all lines which had non-Hawaiian characters. I passed the grep the
-v flag, which inverts the selection, leaving me with only the lines which
contained Hawaiian characters. Finally, I sorted the wordlist using the sort
command with the -u option to remove any duplicates.


Full content of the buildwords script follows:

#!/bin/bash

# get everything inside of <td>s (including the <td>)
grep -o '<td>.*</td>' | \

# remove all backslashes
sed 's/\///g' | \

# remove all <td>s
sed 's/<td>//g' | \

# remove empty lines
sed '/^$/d' | \

# get only even lines
awk 'NR % 2 == 0' | \

# remove <u>s
sed 's/<u>//g' | \

# replace backticks with single quotes
sed "s/\`/'/g" | \

# lowercase everything
tr 'A-Z' 'a-z' | \

# remove commas
sed 's/,//g' | \

# replace whitespace characters with newlines
sed 's/ /\n/g' | \

# remove all lines that don't contain Hawaiian characters
grep -v "[^pk'mnwlhaeiou]" | \

# sort alphabetically, remove duplicates
sort -u
